{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import repackage\n",
    "repackage.up()\n",
    "import os.path as osp\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets.nuscenes import NuScenesMultipleRadarMultiSweeps\n",
    "from utils.labelling import get_sps_labels\n",
    "from utils.transforms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_name</th>\n",
       "      <th>first_frame_datetime</th>\n",
       "      <th>days_since_first_recording</th>\n",
       "      <th>hours_since_first_recording</th>\n",
       "      <th>month</th>\n",
       "      <th>split</th>\n",
       "      <th>closest_scenes</th>\n",
       "      <th>closest_scenes_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>scene-0161</td>\n",
       "      <td>1526922443042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0218]</td>\n",
       "      <td>{'scene-0218': {'scene_token': 'febc1800b9ed43...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>scene-0162</td>\n",
       "      <td>1526922463034</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0665, scene-0218, scene-0219]</td>\n",
       "      <td>{'scene-0665': {'scene_token': '45275e709d4a4b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>scene-0163</td>\n",
       "      <td>1526922483050</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0075, scene-0511, scene-0332, scene-045...</td>\n",
       "      <td>{'scene-0075': {'scene_token': '01c3f5e3995640...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>scene-0164</td>\n",
       "      <td>1526922518041</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0513, scene-0091]</td>\n",
       "      <td>{'scene-0513': {'scene_token': 'e333874a12d64a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>scene-0165</td>\n",
       "      <td>1526922555077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031121</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0092, scene-0333, scene-0265, scene-0062]</td>\n",
       "      <td>{'scene-0092': {'scene_token': '7365495b744646...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     scene_name  first_frame_datetime  days_since_first_recording  \\\n",
       "121  scene-0161         1526922443042                           0   \n",
       "122  scene-0162         1526922463034                           0   \n",
       "123  scene-0163         1526922483050                           0   \n",
       "124  scene-0164         1526922518041                           0   \n",
       "125  scene-0165         1526922555077                           0   \n",
       "\n",
       "     hours_since_first_recording month     split  \\\n",
       "121                     0.000000   May  trainval   \n",
       "122                     0.005553   May  trainval   \n",
       "123                     0.011113   May  trainval   \n",
       "124                     0.020833   May  trainval   \n",
       "125                     0.031121   May  trainval   \n",
       "\n",
       "                                        closest_scenes  \\\n",
       "121                                       [scene-0218]   \n",
       "122               [scene-0665, scene-0218, scene-0219]   \n",
       "123  [scene-0075, scene-0511, scene-0332, scene-045...   \n",
       "124                           [scene-0513, scene-0091]   \n",
       "125   [scene-0092, scene-0333, scene-0265, scene-0062]   \n",
       "\n",
       "                                   closest_scenes_data  \n",
       "121  {'scene-0218': {'scene_token': 'febc1800b9ed43...  \n",
       "122  {'scene-0665': {'scene_token': '45275e709d4a4b...  \n",
       "123  {'scene-0075': {'scene_token': '01c3f5e3995640...  \n",
       "124  {'scene-0513': {'scene_token': 'e333874a12d64a...  \n",
       "125  {'scene-0092': {'scene_token': '7365495b744646...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sps_df = pd.read_json('../sps_nuscenes_more_matches_df.json')\n",
    "# sps_df = pd.read_json('../nuscenes_scenes_df.json')\n",
    "sps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_frame = None\n",
    "num_sweeps = 5\n",
    "ref_sensor = None\n",
    "apply_dpr = False\n",
    "filter_points = False\n",
    "dpr_thresh = 0.75\n",
    "\n",
    "data_dir = \"/shared/data/nuScenes/\"\n",
    "sps_maps_dir = \"../output_sw5-dpr0.15-r1_combined_maps/labelled_maps/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sensors = [\"RADAR_FRONT\", \"RADAR_FRONT_LEFT\", \"RADAR_FRONT_RIGHT\", \"RADAR_BACK_LEFT\", \"RADAR_BACK_RIGHT\"]\n",
    "versions = {'trainval': 'v1.0-trainval', 'test': 'v1.0-test'}\n",
    "nuscenes_exp = {\n",
    "    vname : NuScenes(dataroot=data_dir, version=version, verbose=False)\\\n",
    "    for vname,version in versions.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dataset):\n",
    "    num_frames = len(dataset)\n",
    "    local_poses = dataset.local_poses\n",
    "    global_poses = dataset.global_poses\n",
    "    ego_timestamps = dataset.timestamps\n",
    "    all_data = []\n",
    "\n",
    "    for i in range(num_frames):\n",
    "        frame_dict = {}\n",
    "        (pointclouds, sps_scores), calibs, sensors, timestamps = dataset[i]\n",
    "\n",
    "        for sensor, calib, ts, pcl, scores in zip(sensors, calibs, timestamps, pointclouds, sps_scores):\n",
    "            frame_dict[sensor] = {\n",
    "                'calib' : calib,\n",
    "                'timestamp': ts,\n",
    "                'pointcloud': pcl,\n",
    "                'stability_scores': scores\n",
    "            }\n",
    "        frame_dict['ego_pose'] = global_poses[i]\n",
    "        frame_dict['ego_local_pose'] = local_poses[i]\n",
    "        frame_dict['ego_timestamp'] = ego_timestamps[i]\n",
    "        all_data.append(frame_dict)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sps_df.iloc[0]\n",
    "ref_scene_name = row['scene_name']\n",
    "ref_split = row['split']\n",
    "closest_scenes = row['closest_scenes_data']\n",
    "seq = int(ref_scene_name.split(\"-\")[-1])\n",
    "\n",
    "\n",
    "dataset_sequence = NuScenesMultipleRadarMultiSweeps(\n",
    "    data_dir=data_dir,\n",
    "    nusc=nuscenes_exp[ref_split],\n",
    "    sequence=seq,\n",
    "    sensors=sensors,\n",
    "    nsweeps=num_sweeps,\n",
    "    ref_frame=ref_frame,\n",
    "    ref_sensor=ref_sensor,\n",
    "    sps_thresh=0.0,\n",
    "    return_sps_scores=True,\n",
    "    sps_labels_dir=sps_maps_dir,\n",
    "    apply_dpr=apply_dpr,\n",
    "    filter_points=filter_points,\n",
    "    ransac_threshold=dpr_thresh,\n",
    "    reformat_pcl=False\n",
    "\n",
    ")\n",
    "\n",
    "data = extract_data(dataset_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['ego_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['ego_pose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "def plot_pointclouds(sensor_data, ego_pose):\n",
    "    \"\"\"\n",
    "    Plots the pointclouds from all sensors in the global frame with color coding based on stability scores.\n",
    "    \"\"\"\n",
    "    all_points = []\n",
    "    all_scores = []\n",
    "\n",
    "    if type(sensor_data) == list:\n",
    "        for sd, pose in zip(sensor_data, ego_pose):\n",
    "            for sensor, data in sd.items():\n",
    "                if sensor == 'ego_pose' or sensor == 'ego_timestamp' or sensor == 'ego_local_pose':\n",
    "                    continue\n",
    "                calib_matrix = data['calib']\n",
    "                pointcloud = data['pointcloud']\n",
    "                stability_scores = data['stability_scores']\n",
    "                \n",
    "                # Transform points from sensor to ego vehicle frame\n",
    "                ego_frame_points = transform_doppler_points(calib_matrix, pointcloud)\n",
    "                \n",
    "                # Transform points from ego vehicle frame to global frame\n",
    "                global_frame_points = transform_doppler_points(pose, ego_frame_points)\n",
    "                \n",
    "                # Append points and their color based on individual stability scores\n",
    "                all_points.append(global_frame_points)\n",
    "                all_scores.append(stability_scores)\n",
    "    \n",
    "    all_points = np.vstack(all_points)\n",
    "    all_scores = np.concatenate(all_scores)\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    sc = ax.scatter(all_points[:, 0], all_points[:, 1], c=all_scores, cmap='RdYlGn', s=0.5)\n",
    "    \n",
    "    plt.title('Pointclouds from All Sensors in Global Frame')\n",
    "    plt.colorbar(sc)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointclouds(data[:5], [d['ego_pose'] for d in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pointclouds(data[:5], [d['ego_local_pose'] for d in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_sensor_data(data, sequence_name, base_dir):\n",
    "    \"\"\"\n",
    "    Save the concatenated sensor data to disk in the specified structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: The list of sensor data dictionaries.\n",
    "    - sequence_name: The name of the sequence for which data is being saved.\n",
    "    - base_dir: The base directory where the data will be saved.\n",
    "    \"\"\"\n",
    "    # Create directories\n",
    "    sequence_dir = os.path.join(base_dir, \"sequence\", sequence_name)\n",
    "    scans_dir = os.path.join(sequence_dir, \"scans\")\n",
    "    poses_dir = os.path.join(sequence_dir, \"poses\")\n",
    "    local_poses_dir = os.path.join(sequence_dir, \"local_poses\")\n",
    "    labels_dir = os.path.join(sequence_dir, \"labels\")\n",
    "    map_transform_dir = os.path.join(sequence_dir, \"map_transform\")\n",
    "\n",
    "    os.makedirs(scans_dir, exist_ok=True)\n",
    "    os.makedirs(poses_dir, exist_ok=True)\n",
    "    os.makedirs(local_poses_dir, exist_ok=True)\n",
    "    os.makedirs(labels_dir, exist_ok=True)\n",
    "    os.makedirs(map_transform_dir, exist_ok=True)\n",
    "\n",
    "    # Process each element in the data list\n",
    "    for idx, sensor_data in enumerate(data):\n",
    "        combined_pointclouds = []\n",
    "        combined_sps_scores = []\n",
    "        for sensor in sensor_data:\n",
    "            if sensor == 'ego_pose' or sensor == 'ego_timestamp' or sensor == 'ego_local_pose':\n",
    "                continue\n",
    "\n",
    "            calib_matrix = sensor_data[sensor]['calib']\n",
    "            pointclouds = sensor_data[sensor]['pointcloud']\n",
    "            \n",
    "            transformed_points = transform_doppler_points(calib_matrix, pointclouds)\n",
    "            combined_pointclouds.append(transformed_points)\n",
    "            combined_sps_scores.append(sensor_data[sensor]['stability_scores'])\n",
    "\n",
    "        combined_pointcloud = np.vstack(combined_pointclouds)\n",
    "        combined_sps_scores = np.hstack(combined_sps_scores)\n",
    "\n",
    "        assert(combined_sps_scores.shape[0] == combined_pointcloud.shape[0])\n",
    "        \n",
    "        # Save combined pointcloud and ego pose\n",
    "        ego_timestamp = str(np.mean(np.array(sensor_data['ego_timestamp'])))\n",
    "        scan_file = os.path.join(scans_dir, f\"{ego_timestamp}.npy\")\n",
    "        pose_file = os.path.join(poses_dir, f\"{ego_timestamp}.txt\")\n",
    "        local_pose_file = os.path.join(local_poses_dir, f\"{ego_timestamp}.txt\")\n",
    "        label_file = os.path.join(labels_dir, f\"{ego_timestamp}.npy\")\n",
    "\n",
    "        np.save(scan_file, combined_pointcloud)\n",
    "        np.savetxt(pose_file, sensor_data['ego_pose'], delimiter=',')\n",
    "        np.savetxt(local_pose_file, sensor_data['ego_local_pose'], delimiter=',')\n",
    "        np.save(label_file, combined_sps_scores)\n",
    "\n",
    "        # Save a dummy map_transform file if needed\n",
    "        map_transform_file = os.path.join(map_transform_dir, \"map_transform.txt\")\n",
    "        if not os.path.exists(map_transform_file):\n",
    "            dummy_transform = np.eye(4)\n",
    "            np.savetxt(map_transform_file, dummy_transform, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "base_dir = \"/home/umair/workspace/radar_sps_datasets/nuscenes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(base_dir):\n",
    "    shutil.rmtree(base_dir)\n",
    "    os.mkdir(base_dir)\n",
    "    \n",
    "src_maps_dir = \"/home/umair/workspace/radar_auto_labeler/output_sw5-dpr0.15-r2_combined_maps/labelled_maps/\"\n",
    "dst_maps_dir = os.path.join(base_dir, \"maps\")\n",
    "shutil.copytree(src_maps_dir, dst_maps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main loop\n",
    "\n",
    "for i,row in tqdm(sps_df.iterrows(), total=len(sps_df)):\n",
    "    ref_scene_name = row['scene_name']\n",
    "    ref_split = row['split']\n",
    "    closest_scenes = row['closest_scenes_data']\n",
    "    seq = int(ref_scene_name.split(\"-\")[-1])\n",
    "\n",
    "\n",
    "    # dataset_sequence = NuScenesMultipleRadarMultiSweeps(\n",
    "    #     data_dir=data_dir,\n",
    "    #     nusc=nuscenes_exp[ref_split],\n",
    "    #     sequence=seq,\n",
    "    #     sensors=sensors,\n",
    "    #     nsweeps=num_sweeps,\n",
    "    #     ref_frame=ref_frame,\n",
    "    #     ref_sensor=ref_sensor,\n",
    "    #     sps_thresh=0.0,\n",
    "    #     return_sps_scores=True,\n",
    "    #     sps_labels_dir=sps_maps_dir,\n",
    "    #     apply_dpr=False,\n",
    "    #     filter_points=False,\n",
    "    #     ransac_threshold=-1,\n",
    "    #     reformat_pcl=False\n",
    "\n",
    "    # )\n",
    "\n",
    "    # data = extract_data(dataset_sequence)\n",
    "    # save_sensor_data(data, ref_scene_name, base_dir)\n",
    "\n",
    "    for matched_scene, data in closest_scenes.items():\n",
    "        print(matched_scene)\n",
    "        print(data)\n",
    "        assert(0)\n",
    "        ds = NuScenesMultipleRadarMultiSweeps(\n",
    "            data_dir=data,\n",
    "            nusc=nuscenes_exp[data['split']],\n",
    "            sequence=int(matched_scene.split(\"-\")[-1]),\n",
    "            sensors=sensors,\n",
    "            nsweeps=num_sweeps,\n",
    "            ref_frame=ref_frame,\n",
    "            ref_sensor=ref_sensor,\n",
    "            apply_dpr=False,\n",
    "            filter_points=False,\n",
    "            ransac_threshold=-1,\n",
    "            reformat_pcl=False\n",
    "        )\n",
    "        data = extract_data(dataset_sequence)\n",
    "        save_sensor_data(data, ref_scene_name, base_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_maps(sps_maps_dir):\n",
    "    \"\"\"\n",
    "    Combines all .asc maps from the given directory into a single map and writes it to disk.\n",
    "    \n",
    "    Parameters:\n",
    "    - sps_maps_dir: The directory containing the .asc map files.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "\n",
    "    # Iterate through all .asc files in the directory\n",
    "    for file_name in os.listdir(sps_maps_dir):\n",
    "        if file_name.endswith(\".asc\"):\n",
    "            file_path = os.path.join(sps_maps_dir, file_name)\n",
    "            # Read the .asc file\n",
    "            data = np.loadtxt(file_path, skiprows=1)\n",
    "            combined_data.append(data)\n",
    "    \n",
    "    # Combine all the data into a single array\n",
    "    combined_data = np.vstack(combined_data)\n",
    "    \n",
    "    # Determine the output file path one level up from sps_maps_dir\n",
    "    parent_dir = os.path.abspath(os.path.join(sps_maps_dir, os.pardir))\n",
    "    output_file = os.path.join(parent_dir, \"combined_map.asc\")\n",
    "    \n",
    "    # Write the combined data to a new .asc file\n",
    "    np.savetxt(output_file, combined_data, fmt=\"%.6f\")\n",
    "    print(f\"Combined map saved to {output_file}\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_map = combine_maps(sps_maps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_map = np.load('/home/umair/workspace/radar_auto_labeler/notebooks/boston-seaport_sps.asc',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create scatter plot\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax = fig.add_subplot(111)\n",
    "sc = ax.scatter(full_map[:, 0], full_map[:, 1], c=full_map[:,-1], cmap='RdYlGn', s=0.01)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.colorbar(sc)\n",
    "plt.title('Pointclouds from All Sensors in Global Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create scatter plot\n",
    "sps_only_map = full_map[full_map[:,-1] > 0.0]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax = fig.add_subplot(111)\n",
    "sc = ax.scatter(sps_only_map[:, 0], sps_only_map[:, 1], c=sps_only_map[:,-1], cmap='RdYlGn', s=0.01)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "plt.colorbar(sc)\n",
    "plt.title('Pointclouds from All Sensors in Global Frame')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Splits and update SPS/RIT training configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from the JSON file\n",
    "import yaml\n",
    "\n",
    "# Shuffle the dataframe to ensure randomness\n",
    "df_shuffled = sps_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the data into train (80%), val (10%), test (10%)\n",
    "train_split = int(0.8 * len(df_shuffled))\n",
    "val_split = int(0.9 * len(df_shuffled))\n",
    "\n",
    "train_df = df_shuffled.iloc[:train_split]\n",
    "val_df = df_shuffled.iloc[train_split:val_split]\n",
    "test_df = df_shuffled.iloc[val_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 16, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_sps_config(train_df, val_df, test_df, config_file, new_config_file):\n",
    "    \"\"\"\n",
    "    Populates the SPS config YAML file based on the train/val/test splits from the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): DataFrame containing training scene information.\n",
    "    val_df (pd.DataFrame): DataFrame containing validation scene information.\n",
    "    test_df (pd.DataFrame): DataFrame containing test scene information.\n",
    "    config_file (str): Path to the original YAML configuration file.\n",
    "    new_config_file (str): Path where the updated configuration will be saved.\n",
    "    \"\"\"\n",
    "    # Load the YAML config file\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Function to update the config file with scene names and closest scenes\n",
    "    def update_config(df_split, split_type):\n",
    "        scene_names = df_split['scene_name'].tolist()\n",
    "        closest_scenes_list = df_split['closest_scenes'].tolist()\n",
    "        \n",
    "        config['DATA']['MAPS'][split_type] = scene_names\n",
    "        config['DATA']['SPLIT'][split_type] = [scene for closest_scenes in closest_scenes_list for scene in closest_scenes]\n",
    "\n",
    "    # Update config for train, val, and test splits\n",
    "    update_config(train_df, 'TRAIN')\n",
    "    update_config(val_df, 'VAL')\n",
    "    update_config(test_df, 'TEST')\n",
    "\n",
    "    print('train: ', len(config['DATA']['SPLIT']['TRAIN']))\n",
    "    print('val: ', len(config['DATA']['SPLIT']['VAL']))\n",
    "    print('test: ', len(config['DATA']['SPLIT']['TEST']))\n",
    "\n",
    "    # Save the updated config to a new YAML file\n",
    "    # with open(new_config_file, 'w') as file:\n",
    "        # yaml.dump(config, file)\n",
    "\n",
    "    # print(f\"Updated SPS config file saved as {new_config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_rit_config(train_df, val_df, test_df, config_file, new_config_file):\n",
    "    \"\"\"\n",
    "    Populates the RIT config YAML file based on the train/val/test splits from the dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df (pd.DataFrame): DataFrame containing training scene information.\n",
    "    val_df (pd.DataFrame): DataFrame containing validation scene information.\n",
    "    test_df (pd.DataFrame): DataFrame containing test scene information.\n",
    "    config_file (str): Path to the original YAML configuration file.\n",
    "    new_config_file (str): Path where the updated configuration will be saved.\n",
    "    \"\"\"\n",
    "    # Load the YAML config file\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "    # Helper function to convert scene names to integers\n",
    "    def convert_scene_names(df_split):\n",
    "        # Remove 'scene-' and cast to int\n",
    "        scene_names = df_split['scene_name'].apply(lambda x: int(x.replace('scene-', ''))).tolist()\n",
    "        closest_scenes_list = df_split['closest_scenes'].apply(lambda x: [int(scene.replace('scene-', '')) for scene in x]).tolist()\n",
    "        return scene_names, [scene for closest_scenes in closest_scenes_list for scene in closest_scenes]\n",
    "\n",
    "    # Convert train, val, and test scene names\n",
    "    train_maps, train_splits = convert_scene_names(train_df)\n",
    "    val_maps, val_splits = convert_scene_names(val_df)\n",
    "    test_maps, test_splits = convert_scene_names(test_df)\n",
    "\n",
    "    # Update the config with the converted scene names\n",
    "    config['maps']['train'] = train_maps\n",
    "    config['maps']['valid'] = val_maps\n",
    "    config['maps']['test'] = test_maps\n",
    "\n",
    "    config['split']['train'] = train_splits\n",
    "    config['split']['valid'] = val_splits\n",
    "    config['split']['test'] = test_splits\n",
    "\n",
    "    print('train: ', len(train_splits))\n",
    "    print('val: ', len(val_splits))\n",
    "    print('test: ', len(test_splits))\n",
    "\n",
    "    # Save the updated config to a new YAML file\n",
    "    # with open(new_config_file, 'w') as file:\n",
    "        # yaml.dump(config, file)\n",
    "\n",
    "    # print(f\"Updated RIT config file saved as {new_config_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  295\n",
      "val:  40\n",
      "test:  39\n",
      "train:  295\n",
      "val:  40\n",
      "test:  39\n"
     ]
    }
   ],
   "source": [
    "# Populate SPS config with pre-split dataframes\n",
    "sps_config_file = \"/home/umair/workspace/SPS/config/config.yaml\"\n",
    "rit_config_file = \"/home/umair/workspace/rit-master/config/radar_scenes/radar_mapping.yaml\"\n",
    "\n",
    "populate_sps_config(train_df, val_df, test_df, sps_config_file, 'config.yaml')\n",
    "\n",
    "# Populate RIT config with the same splits\n",
    "populate_rit_config(train_df, val_df, test_df, rit_config_file, 'radar_mapping.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar_auto_labeler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

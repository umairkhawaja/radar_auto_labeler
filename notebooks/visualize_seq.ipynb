{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from NuScenes import *\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import LidarPointCloud\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.geometry_utils import view_points, transform_matrix\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import RadarPointCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sps_labels(map, scan_points):\n",
    "    labeled_map_points = map[:, :3]\n",
    "    labeled_map_labels = map[:, -1]\n",
    "\n",
    "    sps_labels = []\n",
    "    for point in scan_points[:, :3]:\n",
    "        distances = np.linalg.norm(labeled_map_points - point, axis=1)\n",
    "        closest_point_idx = np.argmin(distances)\n",
    "        sps_labels.append(labeled_map_labels[closest_point_idx])\n",
    "    sps_labels = np.array(sps_labels)\n",
    "    return sps_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_pointcloud_to_image(nusc,\n",
    "                            sps_map,\n",
    "                            pointsensor_token: str,\n",
    "                            camera_token: str,\n",
    "                            min_dist: float = 1.0):\n",
    "    \"\"\"\n",
    "    Given a point sensor (lidar/radar) token and camera sample_data token, load pointcloud and map it to the image\n",
    "    plane.\n",
    "    :param pointsensor_token: Lidar/radar sample_data token.\n",
    "    :param camera_token: Camera sample_data token.\n",
    "    :param min_dist: Distance from the camera below which points are discarded.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidar intensity instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes. If None\n",
    "        or the list is empty, all classes will be displayed.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    :return (pointcloud <np.float: 2, n)>, coloring <np.float: n>, image <Image>).\n",
    "    \"\"\"\n",
    "\n",
    "    cam = nusc.get('sample_data', camera_token)\n",
    "    pointsensor = nusc.get('sample_data', pointsensor_token)\n",
    "\n",
    "    pcl_path = osp.join(nusc.dataroot, pointsensor['filename'])\n",
    "    pc = RadarPointCloud.from_file(pcl_path)\n",
    "    im = Image.open(osp.join(nusc.dataroot, cam['filename']))\n",
    "    \n",
    "\n",
    "    # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "    # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "    cs_record = nusc.get('calibrated_sensor', pointsensor['calibrated_sensor_token'])\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(cs_record['translation']))\n",
    "\n",
    "\n",
    "    # Second step: transform from ego to the global frame.\n",
    "    poserecord = nusc.get('ego_pose', pointsensor['ego_pose_token'])\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix)\n",
    "    pc.translate(np.array(poserecord['translation']))\n",
    "\n",
    "    global_points = pc.points.T\n",
    "    sps_score = get_sps_labels(sps_map, global_points)\n",
    "\n",
    "    # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "    poserecord = nusc.get('ego_pose', cam['ego_pose_token'])\n",
    "    pc.translate(-np.array(poserecord['translation']))\n",
    "    pc.rotate(Quaternion(poserecord['rotation']).rotation_matrix.T)\n",
    "    \n",
    "    # Fourth step: transform from ego into the camera.\n",
    "    cs_record = nusc.get('calibrated_sensor', cam['calibrated_sensor_token'])\n",
    "    pc.translate(-np.array(cs_record['translation']))\n",
    "    pc.rotate(Quaternion(cs_record['rotation']).rotation_matrix.T)\n",
    "\n",
    "\n",
    "    # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "    # Grab the depths (camera frame z axis points away from the camera).\n",
    "    depths = pc.points[2, :]\n",
    "    coloring = sps_score\n",
    "\n",
    "    # Take the actual picture (matrix multiplication with camera-matrix + renormalization).\n",
    "    points = view_points(pc.points[:3, :], np.array(cs_record['camera_intrinsic']), normalize=True)\n",
    "\n",
    "    # Remove points that are either outside or behind the camera. Leave a margin of 1 pixel for aesthetic reasons.\n",
    "    # Also make sure points are at least 1m in front of the camera to avoid seeing the lidar points on the camera\n",
    "    # casing for non-keyframes which are slightly out of sync.\n",
    "    mask = np.ones(depths.shape[0], dtype=bool)\n",
    "    mask = np.logical_and(mask, depths > min_dist)\n",
    "    mask = np.logical_and(mask, points[0, :] > 1)\n",
    "    mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "    mask = np.logical_and(mask, points[1, :] > 1)\n",
    "    mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "    points = points[:, mask]\n",
    "    coloring = coloring[mask]\n",
    "\n",
    "    return points, coloring, im\n",
    "\n",
    "def render_pointcloud_in_image(nusc,\n",
    "                               sps_map,\n",
    "                                sample_token: str,\n",
    "                                dot_size: int = 5,\n",
    "                                pointsensor_channel: str = 'LIDAR_TOP',\n",
    "                                camera_channel: str = 'CAM_FRONT',\n",
    "                                out_path: str = None,\n",
    "                                ax = None,\n",
    "                                verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Scatter-plots a pointcloud on top of image.\n",
    "    :param sample_token: Sample token.\n",
    "    :param dot_size: Scatter plot dot size.\n",
    "    :param pointsensor_channel: RADAR or LIDAR channel name, e.g. 'LIDAR_TOP'.\n",
    "    :param camera_channel: Camera channel name, e.g. 'CAM_FRONT'.\n",
    "    :param out_path: Optional path to save the rendered figure to disk.\n",
    "    :param render_intensity: Whether to render lidar intensity instead of point depth.\n",
    "    :param show_lidarseg: Whether to render lidarseg labels instead of point depth.\n",
    "    :param filter_lidarseg_labels: Only show lidar points which belong to the given list of classes.\n",
    "    :param ax: Axes onto which to render.\n",
    "    :param show_lidarseg_legend: Whether to display the legend for the lidarseg labels in the frame.\n",
    "    :param verbose: Whether to display the image in a window.\n",
    "    :param lidarseg_preds_bin_path: A path to the .bin file which contains the user's lidar segmentation\n",
    "                                    predictions for the sample.\n",
    "    :param show_panoptic: When set to True, the lidar data is colored with the panoptic labels. When set\n",
    "        to False, the colors of the lidar data represent the distance from the center of the ego vehicle.\n",
    "        If show_lidarseg is True, show_panoptic will be set to False.\n",
    "    \"\"\"\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    \n",
    "    sample_record = nusc.get('sample', sample_token)\n",
    "\n",
    "\n",
    "    # Here we just grab the front camera and the point sensor.\n",
    "    pointsensor_token = nusc.get('sample_data', sample_record['data'][pointsensor_channel])['token'] # sample_record['data'][pointsensor_channel]\n",
    "    camera_token = nusc.get('sample_data', sample_record['data'][camera_channel])['token'] # sample_record['data'][camera_channel]\n",
    "\n",
    "    points, coloring, im = map_pointcloud_to_image(nusc, sps_map, pointsensor_token, camera_token)\n",
    "\n",
    "    # Init axes.\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(9, 16))\n",
    "        fig.canvas.set_window_title(sample_token)\n",
    "    else:  # Set title on if rendering as part of render_sample.\n",
    "        ax.set_title(camera_channel)\n",
    "    ax.imshow(im)\n",
    "    scatter = ax.scatter(points[0, :], points[1, :], c=coloring, s=dot_size, cmap='RdYlGn')\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "\n",
    "    ax.axis('off')\n",
    "    cbar = plt.colorbar(scatter, cax=cax)\n",
    "    cbar.set_label(\"Stability (Max: 1)\")\n",
    "\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path, bbox_inches='tight', pad_inches=0, dpi=200)\n",
    "    if verbose:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_directories(base_dir, scene_name):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, scene_name), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, scene_name, 'frames'), exist_ok=True)\n",
    "\n",
    "def save_plots_for_scene(nusc, base_dir, scene_name, sample_token, sps_map, num_sweeps=5):\n",
    "    sample = nusc.get('sample', sample_token)\n",
    "    frame_index = 0\n",
    "\n",
    "    while sample_token:\n",
    "        fig, (ax_top, ax_bottom) = plt.subplots(2, 1, figsize=(16, 8), dpi=300)\n",
    "        plot_path = os.path.join(base_dir, scene_name, 'frames', f'{frame_index}.png')\n",
    "        plt.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.05)\n",
    "        \n",
    "        top_pos = [0.05, 0.52, 0.9, 0.46]  # [left, bottom, width, height]\n",
    "        bottom_pos = [0.05, 0.02, 0.9, 0.46]  # [left, bottom, width, height]\n",
    "\n",
    "        nusc.render_sample_data(sample['data']['RADAR_FRONT'], nsweeps=num_sweeps, underlay_map=True, with_anns=True, ax=ax_top)\n",
    "        # top_pos = ax_top.get_position()\n",
    "        # ax_top.set_position([top_pos.x0, top_pos.y0, top_pos.width, top_pos.height])\n",
    "        ax_top.set_position(top_pos)\n",
    "\n",
    "        render_pointcloud_in_image(nusc, sps_map, sample['token'], pointsensor_channel='RADAR_FRONT', ax=ax_bottom)\n",
    "        # bottom_pos = ax_bottom.get_position()\n",
    "        # ax_bottom.set_position([bottom_pos.x0, bottom_pos.y0, bottom_pos.width, bottom_pos.height])\n",
    "        ax_bottom.set_position(bottom_pos)\n",
    "        \n",
    "        ax_top.axis('off')\n",
    "        ax_bottom.axis('off')\n",
    "\n",
    "        # ax_top.set_title(f\"Scene {scene_name} | KeyFrame {frame_index}\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Move to next sample\n",
    "        sample_token = sample['next']\n",
    "        if sample_token:\n",
    "            sample = nusc.get('sample', sample_token)\n",
    "        frame_index += 1\n",
    "\n",
    "def create_video_from_plots(base_dir, scene_name):\n",
    "    plot_dir = os.path.join(base_dir, scene_name, 'frames')\n",
    "    num_frames = len(os.listdir(plot_dir))\n",
    "\n",
    "    sample_image = cv2.imread(os.path.join(plot_dir, '0.png'))\n",
    "\n",
    "    \n",
    "    height, width, _ = sample_image.shape\n",
    "    \n",
    "    video_height = height # 2 * size[1] # height1 + height2\n",
    "    video_width = width\n",
    "    \n",
    "    out_video_path = os.path.join(base_dir, scene_name, f'{scene_name}.mp4')\n",
    "    out = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), 5, (video_width, video_height))\n",
    "\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        img = cv2.imread(os.path.join(plot_dir, f'{i}.png'))\n",
    "        out.write(img)\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/shared/data/nuScenes/\"\n",
    "sensors = [\"RADAR_FRONT\", \"RADAR_FRONT_LEFT\", \"RADAR_FRONT_RIGHT\", \"RADAR_BACK_LEFT\", \"RADAR_BACK_RIGHT\"]\n",
    "versions = {'trainval': 'v1.0-trainval', 'test': 'v1.0-test'}\n",
    "nuscenes_exp = {\n",
    "    vname : NuScenes(dataroot=data_dir, version=version, verbose=False)\\\n",
    "    for vname,version in versions.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scene_name</th>\n",
       "      <th>first_frame_datetime</th>\n",
       "      <th>days_since_first_recording</th>\n",
       "      <th>hours_since_first_recording</th>\n",
       "      <th>month</th>\n",
       "      <th>split</th>\n",
       "      <th>closest_scenes</th>\n",
       "      <th>closest_scenes_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>scene-0162</td>\n",
       "      <td>1526922463034</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0665]</td>\n",
       "      <td>{'scene-0665': {'scene_token': '45275e709d4a4b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>scene-0166</td>\n",
       "      <td>1526922575054</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0440]</td>\n",
       "      <td>{'scene-0440': {'scene_token': '64e1ad8a976542...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>scene-0167</td>\n",
       "      <td>1526922613046</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047223</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0079, scene-0653, scene-0442]</td>\n",
       "      <td>{'scene-0079': {'scene_token': '1e4818b3b2354b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>scene-0168</td>\n",
       "      <td>1526922652038</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058055</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0444]</td>\n",
       "      <td>{'scene-0444': {'scene_token': 'a2e3c0a763c04e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>scene-0170</td>\n",
       "      <td>1526922817015</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103882</td>\n",
       "      <td>May</td>\n",
       "      <td>trainval</td>\n",
       "      <td>[scene-0331]</td>\n",
       "      <td>{'scene-0331': {'scene_token': '9d1307e95c524c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     scene_name  first_frame_datetime  days_since_first_recording  \\\n",
       "122  scene-0162         1526922463034                           0   \n",
       "126  scene-0166         1526922575054                           0   \n",
       "127  scene-0167         1526922613046                           0   \n",
       "128  scene-0168         1526922652038                           0   \n",
       "129  scene-0170         1526922817015                           0   \n",
       "\n",
       "     hours_since_first_recording month     split  \\\n",
       "122                     0.005553   May  trainval   \n",
       "126                     0.036670   May  trainval   \n",
       "127                     0.047223   May  trainval   \n",
       "128                     0.058055   May  trainval   \n",
       "129                     0.103882   May  trainval   \n",
       "\n",
       "                           closest_scenes  \\\n",
       "122                          [scene-0665]   \n",
       "126                          [scene-0440]   \n",
       "127  [scene-0079, scene-0653, scene-0442]   \n",
       "128                          [scene-0444]   \n",
       "129                          [scene-0331]   \n",
       "\n",
       "                                   closest_scenes_data  \n",
       "122  {'scene-0665': {'scene_token': '45275e709d4a4b...  \n",
       "126  {'scene-0440': {'scene_token': '64e1ad8a976542...  \n",
       "127  {'scene-0079': {'scene_token': '1e4818b3b2354b...  \n",
       "128  {'scene-0444': {'scene_token': 'a2e3c0a763c04e...  \n",
       "129  {'scene-0331': {'scene_token': '9d1307e95c524c...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sps_df = pd.read_json('nuscenes_scenes_df.json')\n",
    "sps_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [07:29<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "plt.ioff()\n",
    "\n",
    "\n",
    "ref_frame = 'global'\n",
    "num_sweeps = 5\n",
    "ref_sensor = None\n",
    "apply_dpr = True\n",
    "filter_points = False\n",
    "dpr_thresh = 0.5\n",
    "\n",
    "for i,row in tqdm(sps_df.iterrows(), total=len(sps_df)):\n",
    "\n",
    "    ref_scene_name = row['scene_name']\n",
    "    ref_split = row['split']\n",
    "    closest_scenes = row['closest_scenes_data']\n",
    "    seq = int(ref_scene_name.split(\"-\")[-1])\n",
    "\n",
    "    dataset_sequence = NuScenesMultipleRadarMultiSweeps(\n",
    "        data_dir=data_dir,\n",
    "        nusc=nuscenes_exp[ref_split],\n",
    "        sequence=seq,\n",
    "        sensors=sensors,\n",
    "        nsweeps=num_sweeps,\n",
    "        ref_frame=ref_frame,\n",
    "        ref_sensor=ref_sensor,\n",
    "        apply_dpr=apply_dpr,\n",
    "        filter_points=filter_points,\n",
    "        ransac_threshold=dpr_thresh\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    labelled_map_path = f\"labelled_maps/{ref_scene_name}.asc\"\n",
    "    sps_map = np.loadtxt(labelled_map_path, delimiter=' ', skiprows=1)\n",
    "    sample_token = dataset_sequence.scene['first_sample_token']\n",
    "    base_dir = 'output_videos'\n",
    "\n",
    "    setup_directories(base_dir, ref_scene_name)\n",
    "    save_plots_for_scene(nuscenes_exp[ref_split], base_dir, ref_scene_name, sample_token, sps_map)\n",
    "    create_video_from_plots(base_dir, ref_scene_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nuscenes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
